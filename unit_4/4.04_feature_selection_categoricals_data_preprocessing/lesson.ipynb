{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.4: Feature Selection, Data Processing & KNN Algorithm\n",
    "\n",
    "### Lesson Duration: 3 hours\n",
    "\n",
    "> Purpose: The purpose of this lesson is to manage multiple categorical variables with _feature selection techniques_ and to go in detail on data preprocessing techniques for numerical variables. Students will learn about another regression algorithm - the `KNN` (_k nearest neighbor_).\n",
    "\n",
    "---\n",
    "\n",
    "### Setup\n",
    "\n",
    "- All previous set up\n",
    "- Please use the same Jupyter as used in the previous lesson\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "After this lesson, students will be able to:\n",
    "\n",
    "- Perform feature selection for categorical variables\n",
    "- Implement different scaling techniques for numerical variables\n",
    "- Describe how KNN algorithm works\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson 1 key concepts\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "Revisiting feature selection techniques looked at earlier\n",
    "\n",
    "- Chi-square tests for independence of categorical variables\n",
    "\n",
    "**Chi-square Test for Independence of Categorical variables**\n",
    "\n",
    "- Categorical variables: _nominal_ vs. _ordinal_ (a quick overview)\n",
    "- _Chi-square test_ is used to determine if there is a significant relationship between two nominal (categorical) variables. It works well with nominal categorical variables and it doesn't work well with ordinal categorical variables as the test is based on a contingency table (as we would see later) and the order in which frequencies are put in the table, does not change the result of the chi-square test.\n",
    "\n",
    "In the contingency table, the frequency of each category for one nominal variable is compared across the frequencies of categories of the second nominal variable. Here is the code to show how to check the contingency table in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files_for_lesson_and_activities/data_cleaned.pickle', 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>RFA_2</th>\n",
       "      <th>L1E</th>\n",
       "      <th>L1F</th>\n",
       "      <th>L1G</th>\n",
       "      <th>L2E</th>\n",
       "      <th>L2F</th>\n",
       "      <th>L2G</th>\n",
       "      <th>L3D</th>\n",
       "      <th>L3E</th>\n",
       "      <th>L3F</th>\n",
       "      <th>L3G</th>\n",
       "      <th>L4D</th>\n",
       "      <th>L4E</th>\n",
       "      <th>L4F</th>\n",
       "      <th>L4G</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOMAIN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>249</td>\n",
       "      <td>1842</td>\n",
       "      <td>926</td>\n",
       "      <td>283</td>\n",
       "      <td>689</td>\n",
       "      <td>328</td>\n",
       "      <td>114</td>\n",
       "      <td>421</td>\n",
       "      <td>230</td>\n",
       "      <td>113</td>\n",
       "      <td>210</td>\n",
       "      <td>219</td>\n",
       "      <td>108</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>413</td>\n",
       "      <td>2415</td>\n",
       "      <td>1002</td>\n",
       "      <td>436</td>\n",
       "      <td>916</td>\n",
       "      <td>383</td>\n",
       "      <td>205</td>\n",
       "      <td>640</td>\n",
       "      <td>277</td>\n",
       "      <td>153</td>\n",
       "      <td>434</td>\n",
       "      <td>318</td>\n",
       "      <td>163</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C3</th>\n",
       "      <td>300</td>\n",
       "      <td>1620</td>\n",
       "      <td>605</td>\n",
       "      <td>298</td>\n",
       "      <td>558</td>\n",
       "      <td>199</td>\n",
       "      <td>155</td>\n",
       "      <td>410</td>\n",
       "      <td>166</td>\n",
       "      <td>54</td>\n",
       "      <td>319</td>\n",
       "      <td>228</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1</th>\n",
       "      <td>62</td>\n",
       "      <td>419</td>\n",
       "      <td>185</td>\n",
       "      <td>52</td>\n",
       "      <td>157</td>\n",
       "      <td>72</td>\n",
       "      <td>26</td>\n",
       "      <td>113</td>\n",
       "      <td>52</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>782</td>\n",
       "      <td>4041</td>\n",
       "      <td>1532</td>\n",
       "      <td>778</td>\n",
       "      <td>1472</td>\n",
       "      <td>597</td>\n",
       "      <td>397</td>\n",
       "      <td>1060</td>\n",
       "      <td>480</td>\n",
       "      <td>190</td>\n",
       "      <td>742</td>\n",
       "      <td>535</td>\n",
       "      <td>286</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R3</th>\n",
       "      <td>311</td>\n",
       "      <td>1486</td>\n",
       "      <td>441</td>\n",
       "      <td>280</td>\n",
       "      <td>488</td>\n",
       "      <td>178</td>\n",
       "      <td>165</td>\n",
       "      <td>380</td>\n",
       "      <td>143</td>\n",
       "      <td>53</td>\n",
       "      <td>319</td>\n",
       "      <td>200</td>\n",
       "      <td>93</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>410</td>\n",
       "      <td>3627</td>\n",
       "      <td>1884</td>\n",
       "      <td>411</td>\n",
       "      <td>1231</td>\n",
       "      <td>695</td>\n",
       "      <td>198</td>\n",
       "      <td>751</td>\n",
       "      <td>438</td>\n",
       "      <td>193</td>\n",
       "      <td>368</td>\n",
       "      <td>344</td>\n",
       "      <td>208</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>396</td>\n",
       "      <td>2631</td>\n",
       "      <td>1016</td>\n",
       "      <td>401</td>\n",
       "      <td>995</td>\n",
       "      <td>312</td>\n",
       "      <td>184</td>\n",
       "      <td>673</td>\n",
       "      <td>332</td>\n",
       "      <td>126</td>\n",
       "      <td>383</td>\n",
       "      <td>346</td>\n",
       "      <td>191</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>81</td>\n",
       "      <td>633</td>\n",
       "      <td>198</td>\n",
       "      <td>79</td>\n",
       "      <td>205</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>157</td>\n",
       "      <td>68</td>\n",
       "      <td>31</td>\n",
       "      <td>117</td>\n",
       "      <td>89</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>212</td>\n",
       "      <td>1534</td>\n",
       "      <td>753</td>\n",
       "      <td>204</td>\n",
       "      <td>542</td>\n",
       "      <td>273</td>\n",
       "      <td>91</td>\n",
       "      <td>369</td>\n",
       "      <td>197</td>\n",
       "      <td>74</td>\n",
       "      <td>181</td>\n",
       "      <td>142</td>\n",
       "      <td>98</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>636</td>\n",
       "      <td>3711</td>\n",
       "      <td>1356</td>\n",
       "      <td>651</td>\n",
       "      <td>1341</td>\n",
       "      <td>549</td>\n",
       "      <td>305</td>\n",
       "      <td>1023</td>\n",
       "      <td>417</td>\n",
       "      <td>169</td>\n",
       "      <td>643</td>\n",
       "      <td>535</td>\n",
       "      <td>275</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>130</td>\n",
       "      <td>669</td>\n",
       "      <td>202</td>\n",
       "      <td>109</td>\n",
       "      <td>226</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>201</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>148</td>\n",
       "      <td>91</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U1</th>\n",
       "      <td>162</td>\n",
       "      <td>1451</td>\n",
       "      <td>709</td>\n",
       "      <td>207</td>\n",
       "      <td>493</td>\n",
       "      <td>239</td>\n",
       "      <td>63</td>\n",
       "      <td>305</td>\n",
       "      <td>143</td>\n",
       "      <td>76</td>\n",
       "      <td>127</td>\n",
       "      <td>162</td>\n",
       "      <td>79</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U2</th>\n",
       "      <td>148</td>\n",
       "      <td>1048</td>\n",
       "      <td>422</td>\n",
       "      <td>165</td>\n",
       "      <td>366</td>\n",
       "      <td>142</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>105</td>\n",
       "      <td>59</td>\n",
       "      <td>128</td>\n",
       "      <td>117</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U3</th>\n",
       "      <td>140</td>\n",
       "      <td>861</td>\n",
       "      <td>288</td>\n",
       "      <td>121</td>\n",
       "      <td>257</td>\n",
       "      <td>96</td>\n",
       "      <td>69</td>\n",
       "      <td>209</td>\n",
       "      <td>91</td>\n",
       "      <td>30</td>\n",
       "      <td>109</td>\n",
       "      <td>114</td>\n",
       "      <td>67</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U4</th>\n",
       "      <td>79</td>\n",
       "      <td>624</td>\n",
       "      <td>207</td>\n",
       "      <td>72</td>\n",
       "      <td>214</td>\n",
       "      <td>79</td>\n",
       "      <td>43</td>\n",
       "      <td>133</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "RFA_2   L1E   L1F   L1G  L2E   L2F  L2G  L3D   L3E  L3F  L3G  L4D  L4E  L4F  \\\n",
       "DOMAIN                                                                        \n",
       "C1      249  1842   926  283   689  328  114   421  230  113  210  219  108   \n",
       "C2      413  2415  1002  436   916  383  205   640  277  153  434  318  163   \n",
       "C3      300  1620   605  298   558  199  155   410  166   54  319  228  106   \n",
       "R1       62   419   185   52   157   72   26   113   52   24   50   37   24   \n",
       "R2      782  4041  1532  778  1472  597  397  1060  480  190  742  535  286   \n",
       "R3      311  1486   441  280   488  178  165   380  143   53  319  200   93   \n",
       "S1      410  3627  1884  411  1231  695  198   751  438  193  368  344  208   \n",
       "S2      396  2631  1016  401   995  312  184   673  332  126  383  346  191   \n",
       "S3       81   633   198   79   205   53   56   157   68   31  117   89   32   \n",
       "T1      212  1534   753  204   542  273   91   369  197   74  181  142   98   \n",
       "T2      636  3711  1356  651  1341  549  305  1023  417  169  643  535  275   \n",
       "T3      130   669   202  109   226   86   68   201   72   23  148   91   48   \n",
       "U1      162  1451   709  207   493  239   63   305  143   76  127  162   79   \n",
       "U2      148  1048   422  165   366  142   67   230  105   59  128  117   82   \n",
       "U3      140   861   288  121   257   96   69   209   91   30  109  114   67   \n",
       "U4       79   624   207   72   214   79   43   133   46   24   73   78   41   \n",
       "\n",
       "RFA_2   L4G  \n",
       "DOMAIN       \n",
       "C1       61  \n",
       "C2       79  \n",
       "C3       38  \n",
       "R1       12  \n",
       "R2      114  \n",
       "R3       35  \n",
       "S1      108  \n",
       "S2       68  \n",
       "S3       18  \n",
       "T1       42  \n",
       "T2      106  \n",
       "T3       19  \n",
       "U1       43  \n",
       "U2       32  \n",
       "U3       32  \n",
       "U4       22  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contingency table\n",
    "data_crosstab = pd.crosstab(data['DOMAIN'], data['RFA_2'], margins = False)\n",
    "data_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the data in the contingency table we calculate the expected value of the nominal variables. Based on the expected values, the _chi-square test_ statistic is calculated which helps us decide on whether the variables are independent or not. Technically, though the value of the test statistic we are trying to prove or disprove some hypotheses on the independence of categorical variables.\n",
    "\n",
    "- `H0` (_Null Hypothesis_) - assumes that there is no association between the two variables.\n",
    "- `Ha` (_Alternate Hypothesis_) - assumes that there is an association between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.04 Activity 1\n",
    "\n",
    "Keep working with `unit4.csv` that you already have locally.\n",
    "\n",
    "Use the _Chi-Square_ test for measuring the salary differences between men and women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Activity 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could if we grouped the salary into smaller chunks, but it would not be the most appropriate approach as the variable _salary_ is continuous, in this case, a t-test (or other categorical - continuous tests) would be a better option. You can also generate a new categorical variable with values `['HIGH_INCOME', 'LOW_INCOME']`. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['INCOME'] = data['INCOME'].fillna(data['INCOME'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crosstab = pd.crosstab(np.where(data['INCOME'] > 3,'HIGH_INCOME', 'LOW_INCOME'), data['GENDER'], margins = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>GENDER</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HIGH_INCOME</th>\n",
       "      <td>32013</td>\n",
       "      <td>25344</td>\n",
       "      <td>1154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOW_INCOME</th>\n",
       "      <td>16655</td>\n",
       "      <td>11788</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "GENDER           F      M  other\n",
       "row_0                           \n",
       "HIGH_INCOME  32013  25344   1154\n",
       "LOW_INCOME   16655  11788    820"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, do, expected = chi2_contingency(data_crosstab, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.1884826171395"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.313862285843002e-26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#este es el p-value, por lo tanto, las variables estan muy correlacionadas\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 2 key concepts\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "- Working with the established hypothesis\n",
    "\n",
    "**Working with hypothesis testing and stat**\n",
    "\n",
    "- If the observed chi-square test statistic is greater than the critical value (this value is known already based on certain parameters) in the data, the null hypothesis can be rejected.\n",
    "- If the observed chi-square test statistic is lower than the critical value (this value is known already based on certain parameters) in the data, the null hypothesis is accepted (also put as we fail to reject the null hypothesis) ie. based on the statistics we either reject `H0` or we fail to reject `H0`. You can also use the `p` value directly as we will see later in the lesson.\n",
    "\n",
    "Note: This is only a very brief introduction to the concept of hypothesis testing. We will talk about it in the next couple of weeks.\n",
    "\n",
    "- Three important values that we measure in order to calculate the Chi-square test statistic are:\n",
    "\n",
    "      - Degrees of freedom `(r-1)\\*(c-1)` where `r` is the number of rows and `c` is the number of columns\n",
    "      - Actual frequencies\n",
    "      - Expected frequencies\n",
    "\n",
    "- Based on these values we calculate the test statistic that helps us determine if we reject or fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Chi-squared_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "chi2, p, do, expected = chi2_contingency(data_crosstab, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.313862285843002e-26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns 4 results in this order (_chi-square_ statistic, _p_ value, degrees of freedom, expected frequencies matrix). Looking at the _p_ value, it is usually compared against 0.05. We will talk about _p_ value later but now we will just use this to decide on the variables directly.\n",
    "\n",
    "Since in this case, the _p_ value is less than 0.05 we can reject the null hypothesis (that there is no relationship between the two categorical variables); ie. there is a correlation between the two variables. Hence we can drop one of the two columns. In this case, we are going to drop the column `RFA_2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.04 Activity 2\n",
    "\n",
    "Keep using the same dataset.\n",
    "\n",
    "1. Repeat the same steps already done on the class for the columns `STATE` and `DOMAIN` to find the _chi-square_ statistic.\n",
    "2. Check the `p` value to decide to reject or fail to reject the null hypothesis. If you reject the null hypothesis, then drop one of the columns (`STATE` here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           IL\n",
       "1           CA\n",
       "2           NC\n",
       "3           CA\n",
       "4           FL\n",
       "         ...  \n",
       "90564       FL\n",
       "90565    other\n",
       "90566       TX\n",
       "90567       MI\n",
       "90568       NC\n",
       "Name: STATE, Length: 90569, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['STATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crosstab2 = pd.crosstab(data['DOMAIN'], data['STATE'], margins = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, do, expected = chi2_contingency(data_crosstab2, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20873.291505281064"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>STATE</th>\n",
       "      <th>CA</th>\n",
       "      <th>FL</th>\n",
       "      <th>GA</th>\n",
       "      <th>IL</th>\n",
       "      <th>IN</th>\n",
       "      <th>MI</th>\n",
       "      <th>MO</th>\n",
       "      <th>NC</th>\n",
       "      <th>TX</th>\n",
       "      <th>WA</th>\n",
       "      <th>WI</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOMAIN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>1151</td>\n",
       "      <td>771</td>\n",
       "      <td>174</td>\n",
       "      <td>426</td>\n",
       "      <td>87</td>\n",
       "      <td>183</td>\n",
       "      <td>163</td>\n",
       "      <td>400</td>\n",
       "      <td>643</td>\n",
       "      <td>161</td>\n",
       "      <td>55</td>\n",
       "      <td>1579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>1161</td>\n",
       "      <td>1440</td>\n",
       "      <td>249</td>\n",
       "      <td>450</td>\n",
       "      <td>223</td>\n",
       "      <td>273</td>\n",
       "      <td>178</td>\n",
       "      <td>359</td>\n",
       "      <td>499</td>\n",
       "      <td>426</td>\n",
       "      <td>226</td>\n",
       "      <td>2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C3</th>\n",
       "      <td>469</td>\n",
       "      <td>702</td>\n",
       "      <td>176</td>\n",
       "      <td>261</td>\n",
       "      <td>268</td>\n",
       "      <td>314</td>\n",
       "      <td>115</td>\n",
       "      <td>244</td>\n",
       "      <td>538</td>\n",
       "      <td>202</td>\n",
       "      <td>109</td>\n",
       "      <td>1658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1</th>\n",
       "      <td>218</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>56</td>\n",
       "      <td>23</td>\n",
       "      <td>51</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "      <td>115</td>\n",
       "      <td>95</td>\n",
       "      <td>32</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>681</td>\n",
       "      <td>462</td>\n",
       "      <td>567</td>\n",
       "      <td>669</td>\n",
       "      <td>721</td>\n",
       "      <td>832</td>\n",
       "      <td>459</td>\n",
       "      <td>802</td>\n",
       "      <td>972</td>\n",
       "      <td>457</td>\n",
       "      <td>691</td>\n",
       "      <td>5693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R3</th>\n",
       "      <td>166</td>\n",
       "      <td>171</td>\n",
       "      <td>184</td>\n",
       "      <td>198</td>\n",
       "      <td>106</td>\n",
       "      <td>309</td>\n",
       "      <td>312</td>\n",
       "      <td>302</td>\n",
       "      <td>331</td>\n",
       "      <td>36</td>\n",
       "      <td>146</td>\n",
       "      <td>2311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1</th>\n",
       "      <td>3340</td>\n",
       "      <td>724</td>\n",
       "      <td>463</td>\n",
       "      <td>1004</td>\n",
       "      <td>215</td>\n",
       "      <td>582</td>\n",
       "      <td>232</td>\n",
       "      <td>152</td>\n",
       "      <td>1027</td>\n",
       "      <td>397</td>\n",
       "      <td>212</td>\n",
       "      <td>2518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2</th>\n",
       "      <td>1472</td>\n",
       "      <td>842</td>\n",
       "      <td>177</td>\n",
       "      <td>660</td>\n",
       "      <td>259</td>\n",
       "      <td>687</td>\n",
       "      <td>269</td>\n",
       "      <td>71</td>\n",
       "      <td>547</td>\n",
       "      <td>526</td>\n",
       "      <td>285</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3</th>\n",
       "      <td>245</td>\n",
       "      <td>233</td>\n",
       "      <td>36</td>\n",
       "      <td>126</td>\n",
       "      <td>97</td>\n",
       "      <td>217</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>194</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>699</td>\n",
       "      <td>301</td>\n",
       "      <td>344</td>\n",
       "      <td>288</td>\n",
       "      <td>176</td>\n",
       "      <td>216</td>\n",
       "      <td>96</td>\n",
       "      <td>315</td>\n",
       "      <td>384</td>\n",
       "      <td>164</td>\n",
       "      <td>117</td>\n",
       "      <td>1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>906</td>\n",
       "      <td>905</td>\n",
       "      <td>582</td>\n",
       "      <td>547</td>\n",
       "      <td>509</td>\n",
       "      <td>747</td>\n",
       "      <td>324</td>\n",
       "      <td>991</td>\n",
       "      <td>857</td>\n",
       "      <td>431</td>\n",
       "      <td>414</td>\n",
       "      <td>4504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>95</td>\n",
       "      <td>155</td>\n",
       "      <td>100</td>\n",
       "      <td>111</td>\n",
       "      <td>83</td>\n",
       "      <td>109</td>\n",
       "      <td>63</td>\n",
       "      <td>178</td>\n",
       "      <td>202</td>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "      <td>939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U1</th>\n",
       "      <td>2414</td>\n",
       "      <td>209</td>\n",
       "      <td>26</td>\n",
       "      <td>446</td>\n",
       "      <td>6</td>\n",
       "      <td>66</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>328</td>\n",
       "      <td>149</td>\n",
       "      <td>67</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U2</th>\n",
       "      <td>1492</td>\n",
       "      <td>240</td>\n",
       "      <td>13</td>\n",
       "      <td>363</td>\n",
       "      <td>10</td>\n",
       "      <td>152</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>154</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U3</th>\n",
       "      <td>911</td>\n",
       "      <td>360</td>\n",
       "      <td>10</td>\n",
       "      <td>263</td>\n",
       "      <td>14</td>\n",
       "      <td>233</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>37</td>\n",
       "      <td>67</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U4</th>\n",
       "      <td>468</td>\n",
       "      <td>187</td>\n",
       "      <td>18</td>\n",
       "      <td>154</td>\n",
       "      <td>29</td>\n",
       "      <td>294</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>22</td>\n",
       "      <td>58</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "STATE     CA    FL   GA    IL   IN   MI   MO   NC    TX   WA   WI  other\n",
       "DOMAIN                                                                  \n",
       "C1      1151   771  174   426   87  183  163  400   643  161   55   1579\n",
       "C2      1161  1440  249   450  223  273  178  359   499  426  226   2350\n",
       "C3       469   702  176   261  268  314  115  244   538  202  109   1658\n",
       "R1       218    48   32    56   23   51   33   52   115   95   32    530\n",
       "R2       681   462  567   669  721  832  459  802   972  457  691   5693\n",
       "R3       166   171  184   198  106  309  312  302   331   36  146   2311\n",
       "S1      3340   724  463  1004  215  582  232  152  1027  397  212   2518\n",
       "S2      1472   842  177   660  259  687  269   71   547  526  285   2259\n",
       "S3       245   233   36   126   97  217   50   13   194   65   40    501\n",
       "T1       699   301  344   288  176  216   96  315   384  164  117   1612\n",
       "T2       906   905  582   547  509  747  324  991   857  431  414   4504\n",
       "T3        95   155  100   111   83  109   63  178   202   26   31    939\n",
       "U1      2414   209   26   446    6   66   60    4   328  149   67    484\n",
       "U2      1492   240   13   363   10  152   40    2   154   76   48    521\n",
       "U3       911   360   10   263   14  233   75    1   106   37   67    407\n",
       "U4       468   187   18   154   29  294   56    0   143   22   58    306"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_crosstab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['STATE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'STATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'STATE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_crostab2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcrosstab(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSTATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOMAIN\u001b[39m\u001b[38;5;124m'\u001b[39m], margins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'STATE'"
     ]
    }
   ],
   "source": [
    "data_crostab2 = pd.crosstab(data['STATE'], data['DOMAIN'], margins=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crostab2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, do, expected = chi2_contingency(data_crosstab2, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['STATE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### :pencil2: Check for Understanding - Class activity/quick quiz\n",
    "\n",
    "### ~~Lesson 3 key concepts~~\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "- Data Preprocessing for numerical variables\n",
    "- Difference between standardization, normalization and min-max scaling\n",
    "\n",
    "**Description:**\n",
    "\n",
    "- _Standard scaler_: Removes the mean and scales the data to unit variance. For each column, each value in the column is subtracted by the mean of the column and then divided by the standard deviation.\n",
    "- _Min-max scaler_: It scales the data in the range of `[0,1]`. For each column, each value in the column is subtracted by the max of the column and then divided by the difference of max and min of the column. It is very sensitive to the presence of outliers.\n",
    "- _Normalize_: It rescales the vector to have a unit norm. This means that, for each column, each value is divided by the magnitude of the column. The magnitude is calculated as the euclidean distance.\n",
    "\n",
    "More info: [Additional reading](https://stackoverflow.com/questions/39120942/difference-between-standardscaler-and-normalizer-in-sklearn-preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the data we got after using VIF step for feature selection\n",
    "# we will use it for numerical variables\n",
    "data_corr = data[['INCOME', 'HV1', 'HV2', 'IC1', 'IC2', 'IC3', 'IC4', 'NUMPROM', 'CARDPROM', 'NGIFTALL', 'TIMELAG', 'AVGGIFT']]\n",
    "data_corr['INCOME'].fillna(np.mean(data_corr['INCOME']), inplace=True)\n",
    "data_corr['TIMELAG'].fillna(np.mean(data_corr['TIMELAG']), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr.head()\n",
    "numerical = data_corr.drop(['AVGGIFT'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization/Standard Scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "transformer = StandardScaler().fit(data_corr)\n",
    "x_standardized = transformer.transform(data_corr)\n",
    "#x_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max scaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "transformer = MinMaxScaler().fit(data_corr)\n",
    "x_min_max = transformer.transform(data_corr)\n",
    "# x_min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer().fit(data_corr)\n",
    "x_normalized = transformer.transform(data_corr)\n",
    "# x_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.04 Activity 3\n",
    "\n",
    "Keep using the same dataset.\n",
    "\n",
    "Check the distributions of the numerical data (`numerical`) we got and decide which scaler should perform better with it. \n",
    "\n",
    "**Hint**: You can also plot the scaled distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data_corr:\n",
    "    plt.figure(figsize = (14,6))\n",
    "    sns.distplot(data_corr[column])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dfs = [Normalizer().fit_transform(data_corr), StandardScaler().fit_transform(data_corr), MinMaxScaler().fit_transform(data_corr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaled in scaled_dfs:\n",
    "    new_df = pd.DataFrame(scaled)\n",
    "    new_df.columns = data_corr.columns\n",
    "    print(\"Transformation\")\n",
    "    for column in new_df:\n",
    "        plt.figure(figsize = (14,6))\n",
    "        sns.distplot(new_df[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We won't use normalizer because it doesn't work as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 4 key concepts\n",
    "\n",
    "> :clock10: 20 min\n",
    "\n",
    "- Processing categorical column `DOMAIN`\n",
    "- Encoding categorical column `DOMAIN` variables\n",
    "- Introduce the `KNN` algorithm\n",
    "\n",
    "\n",
    "Keep using the same dataset we use in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning categorical column DOMAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_domain = pd.DataFrame(data['DOMAIN'].value_counts())\n",
    "vals_domain = vals_domain.reset_index()\n",
    "vals_domain.columns = ['domain', 'counts']\n",
    "group_vals_domain_df = vals_domain[vals_domain['counts']<5000]\n",
    "group_vals_domain = list(group_vals_domain_df['domain'])\n",
    "group_vals_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vals_domain(x):\n",
    "    if x in group_vals_domain:\n",
    "        return 'other'\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DOMAIN'] = list(map(clean_vals_domain, data['DOMAIN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DOMAIN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DOMAIN'].fillna('other', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='error', drop='first').fit(data[['DOMAIN']])\n",
    "encoded = encoder.transform(data[['DOMAIN']]).toarray()\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.04 Activity 4\n",
    "\n",
    "Keep using the same dataset.\n",
    "\n",
    "Create one of the two scaling methods we have seen (_standard_ and _minmax_) as a Python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution to Activity 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(ls, mean = True , std = True):\n",
    "\n",
    "    array = np.array(ls)\n",
    "    if mean == True:\n",
    "        mean = array.mean()\n",
    "    if std == True:\n",
    "        std = array.std()\n",
    "\n",
    "    return (array - mean) / std, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(ls, minimum = True , maximum = True):\n",
    "\n",
    "    array = np.array(ls)\n",
    "    if minimum == True:\n",
    "        minimum = array.min()\n",
    "    if maximum == True:\n",
    "        maximum = array.max()\n",
    "\n",
    "    return (array - minimum) / (maximum - minimum), minimum, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Data cleaning and wrangling\n",
    "\n",
    "For this lab, we will be using the same dataset we used in the previous labs. We recommend using the same notebook since you will be reusing the same variables you previous created and used in labs. \n",
    "\n",
    "### Instructions\n",
    "\n",
    "So far we have worked on `EDA`. This lab will focus on data cleaning and wrangling from everything we noticed before.\n",
    "\n",
    "1. We will start with removing outliers. So far, we have discussed different methods to remove outliers. Use the one you feel more comfortable with, define a function for that. Use the function to remove the outliers and apply it to the dataframe.\n",
    "2. Create a copy of the dataframe for the data wrangling.\n",
    "3. Normalize the continuous variables. You can use any one method you want.\n",
    "4. Encode the categorical variables\n",
    "5. The time variable can be useful. Try to transform its data into a useful one. Hint: Day week and month as integers might be useful.\n",
    "6. Since the model will only accept numerical data, check and make sure that every column is numerical, if some are not, change it using encoding.\n",
    "\n",
    "**Hint for Categorical Variables**\n",
    "\n",
    "- You should deal with the categorical variables as shown below (for ordinal encoding, dummy code has been provided as well):\n",
    "\n",
    "```python\n",
    "# One hot to state\n",
    "# Ordinal to coverage\n",
    "# Ordinal to employmentstatus\n",
    "# Ordinal to location code\n",
    "# One hot to marital status\n",
    "# One hot to policy type\n",
    "# One hot to policy\n",
    "# One hot to renew offercustomer_df\n",
    "# One hot to sales channel\n",
    "# One hot vehicle class\n",
    "# Ordinal vehicle size\n",
    "\n",
    "data[\"coverage\"] = data[\"coverage\"].map({\"Basic\" : 0, \"Extended\" : 1, \"Premium\" : 2})\n",
    "# given that column \"coverage\" in the dataframe \"data\" has three categories:\n",
    "# \"basic\", \"extended\", and \"premium\" and values are to be represented in the same order.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No NaNs so just remove outliers (normalization method)\n",
    "def outliers(column, threshold = 3):\n",
    "    \"\"\"\n",
    "    docs\n",
    "    \"\"\"\n",
    "\n",
    "    data = column[abs(column.apply(lambda x: (x - column.mean()) / column.var() ** (1/2))) > threshold]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLV_outliers = outliers(customer_df[\"customer_lifetime_value\"])\n",
    "MPA_outliers = outliers(customer_df[\"monthly_premium_auto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = CLV_outliers.index | MPA_outliers.index # Union\n",
    "clean_customer_df = customer_df.drop(to_drop).reset_index(drop = True)\n",
    "clean_customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = clean_customer_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing continuous variables (but target)\n",
    "\n",
    "continuous.remove(\"months_since_policy_inception\")\n",
    "continuous.remove(\"total_claim_amount\")\n",
    "for cont_var in continuous:\n",
    "    maximum = clean_customer_df[cont_var].max()\n",
    "    minimum = clean_customer_df[cont_var].min()\n",
    "    clean_customer_df[cont_var] = clean_customer_df[cont_var].apply(lambda x: (x - minimum) / (maximum - minimum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv(\"WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv\")\n",
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMEMBER**\n",
    "\n",
    "- Education, employment status, policy, and vehicle class are somewhat unbalanced.\n",
    "- For education, we could turn it into a binary variable (`college +-`), but I wouldn't touch it.\n",
    "- The policy is redundant, maybe we can classify it in `L1`, `L2` and `L3` groups.\n",
    "- Id concatenates luxury SUV, sports car and luxury car into luxury or among the other classes.\n",
    "- For employment, we could divide them among employed, unemployed and inactive.\n",
    "- We can see that having open complaints isn't that common, so we can turn it into a binary variable, open - not open.\n",
    "- For the number of policies, we could join use 1, 2, 3, 4+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "# One hot to state\n",
    "# Ordinal to coverage\n",
    "# Ordinal to employmentstatus\n",
    "# Ordinal to location code\n",
    "# One hot to marital status\n",
    "# One hot to policy type\n",
    "# One hot to policy\n",
    "# One hot to renew offercustomer_df\n",
    "# One hot to sales channel\n",
    "# One hot vehicle class\n",
    "# Ordinal vehicle size\n",
    "```\n",
    "\n",
    "```python\n",
    "customer_df.isna().sum()/len(customer_df)\n",
    "clean_customer_df[\"education\"] = clean_customer_df[\"education\"].apply(lambda x: \"Graduate\" if x in [\"Master\", \"Doctor\"] else x)\n",
    "inactive = [\"Medical Leave\", \"Disabled\", \"Retired\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_customer_df[\"employmentstatus\"] = clean_customer_df[\"employmentstatus\"].apply(lambda x: \"Inactive\" if x in inactive else x)\n",
    "clean_customer_df[\"gender\"] = clean_customer_df[\"gender\"].apply(lambda x: 1 if x == \"F\" else 0)\n",
    "clean_customer_df[\"policy\"] = clean_customer_df[\"policy\"].apply(lambda x: x[-2:])\n",
    "luxury = [\"Sports Car\", \"Luxury SUV\", \"Luxury Car\"]\n",
    "clean_customer_df[\"vehicle_class\"] = clean_customer_df[\"vehicle_class\"].apply(lambda x: \"Luxury\" if x in luxury else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "final_df = clean_customer_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop customer (id)\n",
    "ordinal = clean_customer_df.drop(columns = \"customer\")\n",
    "ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoders\n",
    "# Ordinal to coverage\n",
    "# Ordinal to employmentstatus\n",
    "# Ordinal to location code\n",
    "# Ordinal vehicle size\n",
    "\n",
    "ordinal[\"coverage\"] = ordinal[\"coverage\"].map({\"Basic\" : 0, \"Extended\" : 1, \"Premium\" : 2})\n",
    "ordinal[\"employmentstatus\"] = ordinal[\"employmentstatus\"].map({\"Unemployed\" : 0, \"Inactive\" : 1, \"Employed\" : 2})\n",
    "ordinal[\"location_code\"] = ordinal[\"location_code\"].map({\"Rural\" : 0, \"Suburban\" : 1, \"Urban\" : 2})\n",
    "ordinal[\"vehicle_size\"] = ordinal[\"vehicle_size\"].map({\"Small\" : 0, \"Medsize\" : 1, \"Large\" : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = ordinal.copy()\n",
    "one_hot_colums = final_df.select_dtypes(include = object).columns[1:]\n",
    "one_hot_colums\n",
    "\n",
    "# One hot encoders\n",
    "\n",
    "# One hot to state\n",
    "# One hot to marital status\n",
    "# One hot to policy type\n",
    "# One hot to policy\n",
    "# One hot to renew offercustomer_df\n",
    "# One hot to sales channel\n",
    "# One hot vehicle class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(one_hot, columns = one_hot_colums)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = one_hot.copy()\n",
    "final_df[\"day\"] = time_df[\"day\"]\n",
    "final_df[\"week\"] = time_df[\"week\"]\n",
    "final_df[\"month\"] = time_df[\"month\"]\n",
    "final_df = final_df.drop(columns = \"effective_to_date\")\n",
    "final_df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "- [Difference between `standardscaler` and Normalizer in `sklearn.preprocessing`](https://stackoverflow.com/questions/39120942/difference-between-standardscaler-and-normalizer-in-sklearn-preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
